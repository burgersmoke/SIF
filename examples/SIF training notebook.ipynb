{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Purpose of this notebook is to train Smooth Inverse Frequency (SIF) embeddings but in a notebook so that long-loading tasks like loading embeddings or training models can be simplified so that we may be able to wrap this into a class and serialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "print('gensim version : {}'.format(gensim.__version__))\n",
    "\n",
    "import spacy\n",
    "print('Spacy version : {}'.format(spacy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "from SIF_embedding import get_weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io_w2v import load_w2v_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "#wordfile = '../data/glove.840B.300d.txt' # word vector file, can be downloaded from GloVe website\n",
    "wordfile = r'C:/temp_embeddings/pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin'\n",
    "\n",
    "#EMBEDDINGS_FORMAT = 'GLOVE'\n",
    "EMBEDDINGS_FORMAT = 'WORD2VEC_BIN'\n",
    "\n",
    "weightfile = '../auxiliary_data/enwiki_vocab_min200.txt' # each line is a word and its frequency\n",
    "\n",
    "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "rmpc = 1 # number of principal components to remove in SIF weighting scheme\n",
    "sentences = ['this is an example sentence', 'this is another sentence that is slightly longer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load word vectors\n",
    "#(words, We) = data_io.getWordmap(wordfile)\n",
    "\n",
    "words = None\n",
    "We = None\n",
    "if EMBEDDINGS_FORMAT == 'GLOVE':\n",
    "    print('Loading embeddings as GLOVE')\n",
    "    (words, We) = data_io.load_glove_word_map(wordfile)\n",
    "elif EMBEDDINGS_FORMAT == 'WORD2VEC_BIN':\n",
    "    (words, We) = load_w2v_word_map(wordfile, binary = True)\n",
    "elif EMBEDDINGS_FORMAT == 'WORD2VEC_TXT':\n",
    "    (words, We) = load_w2v_word_map(wordfile, binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word weights\n",
    "word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "x, m = data_io.sentences2idx(sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = data_io.seq2weight(x, m, weight4ind) # get word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "sif_params = params.params()\n",
    "sif_params.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embedding = SIF_embedding.SIF_embedding(We, x, w, sif_params) # embedding[i,:] is the embedding for sentence i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's load some other sentences from a PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_PICKLE_PATH = '../data/MIMIC_DISCHARGE_SUMMARIES.pickle'\n",
    "#mimic_file = open(MIMIC_PICKLE_PATH, 'rb')\n",
    "#mimic_df = pickle.load(mimic_file)\n",
    "#mimic_file.close()\n",
    "\n",
    "mimic_df = pd.read_pickle(MIMIC_PICKLE_PATH)\n",
    "\n",
    "print(type(mimic_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mimic_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mimic_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_DOCUMENTS_FOR_TRAINING = 10000\n",
    "MAX_DOCUMENTS_FOR_TRAINING = 100\n",
    "MAX_TOKENS_PER_SENTENCE = 30\n",
    "MIN_TOKENS_PER_SENTENCE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mimic_texts = mimic_df.TEXT.unique()[:MAX_DOCUMENTS_FOR_TRAINING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mimic_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mimic_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load spacy and get ready to tokenize for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Processing and gathering sentences...')\n",
    "\n",
    "training_sentences = []\n",
    "for mimic_text in mimic_texts:\n",
    "    doc = nlp(mimic_text)\n",
    "    # loop through sentences\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        if len(sent) < MIN_TOKENS_PER_SENTENCE:\n",
    "            continue\n",
    "            \n",
    "        if len(sent) > MAX_TOKENS_PER_SENTENCE:\n",
    "            continue\n",
    "        \n",
    "        tokens = sent[0 : MAX_TOKENS_PER_SENTENCE]\n",
    "        sentence_str = ' '.join(token.text for token in tokens)\n",
    "        training_sentences.append(sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total training sentences : {}'.format(len(training_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try to train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "x, m = data_io.sentences2idx(training_sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = data_io.seq2weight(x, m, weight4ind) # get word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters\n",
    "sif_params = params.params()\n",
    "sif_params.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embedding = SIF_embedding.SIF_embedding(We, x, w, sif_params) # embedding[i,:] is the embedding for sentence i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIFModel(object):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "        self.svd = None\n",
    "        self.word_map = None\n",
    "        self.params = params\n",
    "        \n",
    "    def save(self, filename):\n",
    "        components = [self.word_map, self.weight4ind, self.params, self.svd]\n",
    "        joblib.dump(components, filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        components = joblib.load(filename)\n",
    "        self.trained = True\n",
    "        self.word_map = components[0]\n",
    "        self.weight4ind = components[1]\n",
    "        self.params = components[2]\n",
    "        self.svd = components[3]\n",
    "\n",
    "    def transform(self, We, sentences):\n",
    "        x, m = data_io.sentences2idx(sentences, self.word_map) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "        w = data_io.seq2weight(x, m, self.weight4ind) # get word weights\n",
    "        weighted_emb = get_weighted_average(We, x, w)\n",
    "        # now use the model we've already loaded\n",
    "        return self.remove_pc(weighted_emb)\n",
    "        \n",
    "    def compute_pc(self, X):\n",
    "        # this is what happens in compute_pc() in src/SIF_embedding.py\n",
    "        self.svd = TruncatedSVD(n_components=self.params.rmpc, n_iter=7, random_state=0)\n",
    "        self.svd.fit(X)\n",
    "        \n",
    "    def remove_pc(self, X):\n",
    "        pc = self.svd.components_\n",
    "        \n",
    "        if self.params.rmpc == 1:\n",
    "            XX = X - X.dot(pc.transpose()) * pc\n",
    "        else:\n",
    "            XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "            \n",
    "        return XX\n",
    "        \n",
    "    def fit(self, sentences, We, params, word_map, weight4ind):\n",
    "        \n",
    "        # store these off for pickling or extra transforms\n",
    "        self.word_map = word_map\n",
    "        self.weight4ind = weight4ind\n",
    "        self.params = params\n",
    "        \n",
    "        x, m = data_io.sentences2idx(training_sentences, self.word_map) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "        w = data_io.seq2weight(x, m, self.weight4ind) # get word weights\n",
    "        \n",
    "        # now let's do some of what happens in src/SIF_embedding.py\n",
    "        # but also keep some pieces along the way\n",
    "        weighted_emb = get_weighted_average(We, x, w)\n",
    "        \n",
    "        self.compute_pc(weighted_emb)\n",
    "        \n",
    "        self.trained = True\n",
    "        \n",
    "        return self.remove_pc(weighted_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIF filename\n",
    "SIF_JOBLIB_FILE_NAME = 'SIF_{0}.joblib'.format(os.path.splitext(os.path.basename(wordfile))[0])\n",
    "\n",
    "print('Preparing to train a model to be stored at: {}'.format(SIF_JOBLIB_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sif_model = SIFModel()\n",
    "\n",
    "# now let's train it...\n",
    "model_embeddings = sif_model.fit(training_sentences, We, sif_params, words, weight4ind)\n",
    "print(model_embeddings[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sif_model.save(SIF_JOBLIB_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_sif_model = SIFModel()\n",
    "loaded_sif_model.load(SIF_JOBLIB_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings = loaded_sif_model.transform(We, [training_sentences[0]])\n",
    "print(loaded_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n2c2] *",
   "language": "python",
   "name": "conda-env-n2c2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
