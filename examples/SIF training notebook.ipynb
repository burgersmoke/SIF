{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Purpose of this notebook is to train Smooth Inverse Frequency (SIF) embeddings but in a notebook so that long-loading tasks like loading embeddings or training models can be simplified so that we may be able to wrap this into a class and serialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\n2c2\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "c:\\anaconda3\\envs\\n2c2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version : 3.4.0\n",
      "Spacy version : 2.0.18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# give this a different alias so that it does not conflict with SPACY\n",
    "from sklearn.externals import joblib as sklearn_joblib\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "print('gensim version : {}'.format(gensim.__version__))\n",
    "\n",
    "import spacy\n",
    "print('Spacy version : {}'.format(spacy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "from SIF_embedding import get_weighted_average\n",
    "from sif_model import SIFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "#wordfile = '../data/glove.840B.300d.txt' # word vector file, can be downloaded from GloVe website\n",
    "wordfile = r'C:/temp_embeddings/pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin'\n",
    "\n",
    "#EMBEDDINGS_FORMAT = 'GLOVE'\n",
    "EMBEDDINGS_FORMAT = 'WORD2VEC_BIN'\n",
    "\n",
    "# this behavior may change between sets of embeddings since tokens may be all lowercased or case may be intact\n",
    "LOWERCASE_TOKENS = True\n",
    "\n",
    "weightfile = '../auxiliary_data/enwiki_vocab_min200.txt' # each line is a word and its frequency\n",
    "\n",
    "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "rmpc = 1 # number of principal components to remove in SIF weighting scheme\n",
    "sentences = ['this is an example sentence', 'this is another sentence that is slightly longer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec formatted embeddings from [C:/temp_embeddings/pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin] with binary=True\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load word vectors\n",
    "#(words, We) = data_io.getWordmap(wordfile)\n",
    "\n",
    "words, We = SIFModel.embedding_loading_helper(wordfile, EMBEDDINGS_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word weights\n",
    "word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "x, m = data_io.sentences2idx(sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = data_io.seq2weight(x, m, weight4ind) # get word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "sif_params = params.params()\n",
    "sif_params.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embedding = SIF_embedding.SIF_embedding(We, x, w, sif_params) # embedding[i,:] is the embedding for sentence i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 200)\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.11440651e-01  4.49017632e-01  3.75954538e-02  1.43402284e-01\n",
      "  -1.66645969e-01  3.02967699e-03  4.92990278e-02 -3.07121700e-01\n",
      "   1.42688153e-01  2.70598224e-01  3.17469827e-01 -8.91577591e-02\n",
      "  -2.74349888e-01 -1.06764611e-01 -4.45135944e-02  1.57391157e-01\n",
      "  -3.42903763e-02 -1.84726822e-01  6.22194217e-02  3.21345152e-01\n",
      "   1.34034157e-02  2.27410475e-02  1.78331967e-01 -1.08391695e-01\n",
      "  -1.25103904e-01  6.54320606e-02  1.92067426e-01 -3.23984721e-02\n",
      "   2.28680329e-01  4.44462385e-02  1.75134811e-01 -8.90729775e-02\n",
      "  -1.47496216e-01 -6.12644060e-03  1.43976993e-01  1.64085490e-01\n",
      "  -3.21689767e-01 -1.04377982e-01  7.43296531e-02  2.71703210e-01\n",
      "   3.29077012e-01  1.04614446e-01  5.07722957e-02  3.89147501e-01\n",
      "  -2.16672833e-03 -3.00373809e-01  2.09553575e-01  4.37169072e-02\n",
      "  -1.92826213e-02 -7.68033348e-02 -5.68043184e-02  2.93115966e-01\n",
      "  -2.44672315e-02  1.64577363e-01 -1.06934870e-01  1.67254890e-01\n",
      "   1.16924524e-01 -1.42886363e-01 -8.39806456e-02  1.22895712e-01\n",
      "   3.44653747e-01  1.53371393e-01 -2.37514993e-01 -4.06021175e-02\n",
      "   2.37486725e-01 -2.76381312e-01  4.41719735e-02 -2.76444038e-01\n",
      "   2.00287657e-01  1.90689742e-01 -3.97731559e-01 -1.56950107e-01\n",
      "   3.66178122e-01  8.83873109e-02 -9.92103552e-02 -1.43183844e-01\n",
      "   1.64412538e-01  1.91268803e-01 -4.11648955e-02  2.08163585e-01\n",
      "   8.71721837e-02 -1.88367906e-01  9.66361288e-02  2.13112473e-01\n",
      "  -1.24046376e-01 -5.08282499e-04  4.16052100e-03 -3.06811664e-01\n",
      "  -2.52386725e-01  1.88942101e-01  4.92582724e-01 -2.65107951e-01\n",
      "  -1.44523849e-01 -1.80516845e-01 -1.06107396e-01 -2.40632605e-02\n",
      "  -6.90298010e-02 -9.46667642e-02 -1.65451815e-01  4.14058814e-01\n",
      "  -2.37560521e-01 -3.09590914e-01 -2.43622013e-01  1.28071168e-01\n",
      "   2.04865593e-01  1.22993695e-01  5.03455844e-02  7.92903501e-03\n",
      "  -1.99399956e-01 -2.30258883e-01  8.47063391e-02 -1.52001073e-01\n",
      "   6.48809601e-02  3.95057232e-02  4.05448370e-01 -8.34712852e-02\n",
      "   2.10521454e-02  3.65611713e-02 -3.40383007e-01  3.95259098e-01\n",
      "   4.82291853e-02 -3.92985447e-01 -5.39534425e-02  2.56562158e-01\n",
      "   6.41131947e-02  3.40390872e-01 -4.31317052e-01 -5.39419109e-02\n",
      "   3.45340645e-01  6.04700445e-02  2.55978061e-02  1.48069519e-01\n",
      "  -1.16719948e-01 -1.91356307e-01 -2.45523607e-02 -4.81096145e-03\n",
      "  -2.05376921e-01  7.76164600e-02  1.44236191e-01 -2.64709155e-01\n",
      "   8.59619992e-02  1.28591274e-01 -1.45007921e-01  9.42087486e-02\n",
      "   1.89263723e-01 -2.47626066e-01 -1.24145273e-01 -3.42324415e-01\n",
      "   5.53074543e-02  1.97734934e-02 -7.62079716e-02  2.96973152e-01\n",
      "  -6.61036102e-02 -4.27786104e-02 -2.15747535e-01 -1.28927719e-01\n",
      "  -3.64129980e-01  5.43921888e-02 -6.13813551e-02 -1.78326537e-01\n",
      "   3.85540476e-02 -2.55364636e-01 -3.36265449e-02 -2.14455999e-01\n",
      "   3.52232967e-01 -8.74977045e-02  2.14522577e-01  2.83233793e-01\n",
      "  -3.65775236e-01 -3.93011414e-01  3.68300215e-01 -3.40894434e-01\n",
      "   7.23853740e-03  9.98236276e-02 -1.42182754e-01  3.84552206e-02\n",
      "   7.17535906e-02  7.58413360e-02 -3.73855918e-01 -7.95601363e-02\n",
      "   7.03011401e-02  7.29109101e-02  2.28943482e-02 -1.53373994e-01\n",
      "  -8.99790000e-02  2.31080433e-01  1.55185062e-01  1.39556441e-01\n",
      "  -5.88395826e-02 -5.04877321e-02 -2.32090865e-01  5.85765452e-02\n",
      "   3.29406593e-01  4.65337374e-02  1.29901083e-01  1.07536410e-01\n",
      "  -7.11915177e-02  1.82891679e-01 -8.59402639e-02 -5.87674201e-03]\n",
      " [-3.31082760e-01 -4.77336521e-01 -3.99665444e-02 -1.52446458e-01\n",
      "   1.77156088e-01 -3.22075432e-03 -5.24082458e-02  3.26491420e-01\n",
      "  -1.51687287e-01 -2.87664461e-01 -3.37492188e-01  9.47808094e-02\n",
      "   2.91652736e-01  1.13498100e-01  4.73210021e-02 -1.67317589e-01\n",
      "   3.64530204e-02  1.96377275e-01 -6.61435102e-02 -3.41611923e-01\n",
      "  -1.42487496e-02 -2.41752924e-02 -1.89579105e-01  1.15227802e-01\n",
      "   1.32994025e-01 -6.95587656e-02 -2.04180839e-01  3.44417966e-02\n",
      "  -2.43102865e-01 -4.72493982e-02 -1.86180309e-01  9.46906807e-02\n",
      "   1.56798588e-01  6.51282630e-03 -1.53057413e-01 -1.74434123e-01\n",
      "   3.41978273e-01  1.10960950e-01 -7.90175163e-02 -2.88839137e-01\n",
      "  -3.49831420e-01 -1.11212328e-01 -5.39744306e-02 -4.13690468e-01\n",
      "   2.30338074e-03  3.19317949e-01 -2.22769814e-01 -4.64740689e-02\n",
      "   2.04987482e-02  8.16472094e-02  6.03868841e-02 -3.11602364e-01\n",
      "   2.60103442e-02 -1.74957019e-01  1.13679097e-01 -1.77803413e-01\n",
      "  -1.24298783e-01  1.51897998e-01  8.92771828e-02 -1.30646566e-01\n",
      "  -3.66390558e-01 -1.63044303e-01  2.52494719e-01  4.31628340e-02\n",
      "  -2.52464667e-01  2.93812280e-01 -4.69578356e-02  2.93878961e-01\n",
      "  -2.12919509e-01 -2.02716266e-01  4.22815911e-01  1.66848722e-01\n",
      "  -3.89272444e-01 -9.39617701e-02  1.05467408e-01  1.52214241e-01\n",
      "  -1.74781798e-01 -2.03331849e-01  4.37611056e-02 -2.21292159e-01\n",
      "  -9.26700066e-02  2.00247996e-01 -1.02730829e-01 -2.26553167e-01\n",
      "   1.31869801e-01  5.40339137e-04 -4.42291901e-03  3.26161830e-01\n",
      "   2.68304389e-01 -2.00858404e-01 -5.23649201e-01  2.81827924e-01\n",
      "   1.53638758e-01  1.91901780e-01  1.12799435e-01  2.55808954e-02\n",
      "   7.33834102e-02  1.00637259e-01  1.75886620e-01 -4.40172901e-01\n",
      "   2.52543117e-01  3.29116363e-01  2.58986899e-01 -1.36148431e-01\n",
      "  -2.17786168e-01 -1.30750729e-01 -5.35208072e-02 -8.42910773e-03\n",
      "   2.11975821e-01  2.44780976e-01 -9.00486448e-02  1.61587560e-01\n",
      "  -6.89729079e-02 -4.19972917e-02 -4.31019409e-01  8.87356978e-02\n",
      "  -2.23798736e-02 -3.88670312e-02  3.61850468e-01 -4.20187515e-01\n",
      "  -5.12709299e-02  4.17770468e-01  5.73562077e-02 -2.72743160e-01\n",
      "  -6.81567207e-02 -3.61858830e-01  4.58519591e-01  5.73439488e-02\n",
      "  -3.67120778e-01 -6.42838023e-02 -2.72122225e-02 -1.57408048e-01\n",
      "   1.24081305e-01  2.03424872e-01  2.61008424e-02  5.11438180e-03\n",
      "   2.18329745e-01 -8.25116173e-02 -1.53332958e-01  2.81403977e-01\n",
      "  -9.13834975e-02 -1.36701338e-01  1.54153359e-01 -1.00150357e-01\n",
      "  -2.01200311e-01  2.63243482e-01  1.31974934e-01  3.63914318e-01\n",
      "  -5.87956150e-02 -2.10205789e-02  8.10142976e-02 -3.15702817e-01\n",
      "   7.02726688e-02  4.54765951e-02  2.29354418e-01  1.37059002e-01\n",
      "   3.87095129e-01 -5.78226250e-02  6.52525879e-02  1.89573333e-01\n",
      "  -4.09855954e-02  2.71470112e-01  3.57473222e-02  2.27981428e-01\n",
      "  -3.74447788e-01  9.30160575e-02 -2.28052204e-01 -3.01096936e-01\n",
      "   3.88844149e-01  4.17798073e-01 -3.91528375e-01  3.62394151e-01\n",
      "  -7.69506144e-03 -1.06119359e-01  1.51150014e-01 -4.08805355e-02\n",
      "  -7.62789853e-02 -8.06245388e-02  3.97434467e-01  8.45778784e-02\n",
      "  -7.47349308e-02 -7.75092952e-02 -2.43382614e-02  1.63047069e-01\n",
      "   9.56538448e-02 -2.45654340e-01 -1.64972358e-01 -1.48358062e-01\n",
      "   6.25505095e-02  5.36719200e-02  2.46728499e-01 -6.22708828e-02\n",
      "  -3.50181788e-01 -4.94685526e-02 -1.38093756e-01 -1.14318575e-01\n",
      "   7.56814632e-02 -1.94426391e-01  9.13603914e-02  6.24737959e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's load some other sentences from a PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "MIMIC_PICKLE_PATH = '../data/MIMIC_DISCHARGE_SUMMARIES.pickle'\n",
    "#mimic_file = open(MIMIC_PICKLE_PATH, 'rb')\n",
    "#mimic_df = pickle.load(mimic_file)\n",
    "#mimic_file.close()\n",
    "\n",
    "mimic_df = pd.read_pickle(MIMIC_PICKLE_PATH)\n",
    "\n",
    "print(type(mimic_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55177\n"
     ]
    }
   ],
   "source": [
    "print(len(mimic_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROW_ID                                               TEXT\n",
      "0       1  Admission Date:  [**2823-9-29**]              ...\n",
      "1       2  Admission Date:  [**2830-3-12**]              ...\n",
      "2       3  Admission Date:  [**2714-3-12**]              ...\n",
      "3       4  Admission Date:  [**2678-10-4**]              ...\n",
      "4       5  Admission Date:  [**2936-5-6**]     Discharge ...\n"
     ]
    }
   ],
   "source": [
    "print(mimic_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_MIMIC_DOCUMENTS_FOR_TRAINING = 10000\n",
    "MAX_MIMIC_DOCUMENTS_FOR_TRAINING = 100\n",
    "MAX_TOKENS_PER_SENTENCE = 30\n",
    "MIN_TOKENS_PER_SENTENCE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mimic_texts = mimic_df.TEXT.unique()[:MAX_MIMIC_DOCUMENTS_FOR_TRAINING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(mimic_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mimic_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load spacy and get ready to tokenize for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\"])\n",
    "\n",
    "# let's do a fast sentence breaking WITHOUT a dependency parse...\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])\n",
    "\n",
    "# per spacy docs, this is a \"rule-based sentence segmentation without the dependency parse.\"\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and gathering sentences...\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Processing and gathering sentences...')\n",
    "\n",
    "ENABLE_SPACY_MULTIPROCESSING = False\n",
    "\n",
    "def get_whitespace_sentences(batch_id, texts):\n",
    "    # let's do a fast sentence breaking WITHOUT a dependency parse...\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])\n",
    "\n",
    "    # per spacy docs, this is a \"rule-based sentence segmentation without the dependency parse.\"\n",
    "    sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "    nlp.add_pipe(sentencizer)\n",
    "    \n",
    "    whitespace_sentences = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        # loop through sentences\n",
    "        for sent in doc.sents:\n",
    "\n",
    "            if len(sent) < MIN_TOKENS_PER_SENTENCE:\n",
    "                continue\n",
    "\n",
    "            if len(sent) > MAX_TOKENS_PER_SENTENCE:\n",
    "                continue\n",
    "\n",
    "            tokens = sent[0 : MAX_TOKENS_PER_SENTENCE]\n",
    "            sentence_str = ' '.join(token.text for token in tokens)\n",
    "            whitespace_sentences.append(sentence_str)\n",
    "            \n",
    "    return whitespace_sentences\n",
    "\n",
    "if ENABLE_SPACY_MULTIPROCESSING:\n",
    "    \n",
    "    from joblib import Parallel, delayed\n",
    "    from functools import partial\n",
    "    from spacy.util import minibatch\n",
    "    import multiprocessing\n",
    "    \n",
    "    SPACY_JOBS = multiprocessing.cpu_count() - 2\n",
    "    SPACY_BATCH_SIZE = 10\n",
    "    \n",
    "    print('Starting multiprocessing text processing...')\n",
    "    \n",
    "    partitions = minibatch(mimic_texts, size=SPACY_BATCH_SIZE)\n",
    "    executor = Parallel(n_jobs=SPACY_JOBS, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "    do = delayed(get_whitespace_sentences)\n",
    "    tasks = (do(i, batch) for i, batch in enumerate(partitions))\n",
    "    training_sentences = executor(tasks)\n",
    "\n",
    "    print('Total training sentences : {}'.format(len(training_sentences)))\n",
    "    \n",
    "else:\n",
    "    training_sentences = []\n",
    "    for i, doc in enumerate(nlp.pipe(mimic_texts, batch_size=100)):\n",
    "        # loop through sentences\n",
    "        for sent in doc.sents:\n",
    "\n",
    "            if len(sent) < MIN_TOKENS_PER_SENTENCE:\n",
    "                continue\n",
    "\n",
    "            if len(sent) > MAX_TOKENS_PER_SENTENCE:\n",
    "                continue\n",
    "\n",
    "            tokens = sent[0 : MAX_TOKENS_PER_SENTENCE]\n",
    "            sentence_str = ' '.join(token.text for token in tokens)\n",
    "            \n",
    "            if LOWERCASE_TOKENS:\n",
    "                sentence_str = sentence_str.lower()\n",
    "            \n",
    "            training_sentences.append(sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences : 6846\n"
     ]
    }
   ],
   "source": [
    "print('Total training sentences : {}'.format(len(training_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the patient is \\n in distress , rigoring and has aphasia and only limited history \\n is obtained .', 'she \\n states that headaches are unusual for her .', 'head ct was done and relealved attenuation \\n within the subcortical white matter of the right medial frontal \\n lobe .', 'lp was performed showing opening pressure 24 cm h2o wbc of \\n 316 , protein 152 , glucose 16 .', '  the patient was evaluated by neuro in the \\n ed .']\n"
     ]
    }
   ],
   "source": [
    "print(training_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try to train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "x, m = data_io.sentences2idx(training_sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "w = data_io.seq2weight(x, m, weight4ind) # get word weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set parameters\n",
    "sif_params = params.params()\n",
    "sif_params.rmpc = rmpc\n",
    "# get SIF embedding\n",
    "embedding = SIF_embedding.SIF_embedding(We, x, w, sif_params) # embedding[i,:] is the embedding for sentence i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6846, 200)\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17626167  0.23711565  0.08537466 -0.01195529  0.12861061 -0.13272128\n",
      " -0.19920165  0.08112462  0.05974769 -0.01452875  0.44087513 -0.43310128\n",
      " -0.05488625 -0.02537395  0.19309921 -0.13808011  0.31721966 -0.53458463\n",
      "  0.05382784 -0.32087492 -0.23657945  0.18079587  0.17358089 -0.24244937\n",
      "  0.01254969  0.1371328   0.22613152  0.07706336  0.11525739  0.10930948\n",
      " -0.59057764  0.37499111  0.30563124 -0.30139809 -0.25504244  0.18107983\n",
      " -0.25743415  0.46330456  0.22119576 -0.18447893  0.46111501  0.30915154\n",
      "  0.03020038  0.19233093 -0.0119602  -0.05628163 -0.10377176 -0.21680597\n",
      " -0.28717111  0.19528575 -0.26908734  0.16856631 -0.14822798  0.00387527\n",
      " -0.29648486  0.10642594 -0.01181775  0.23163724 -0.07085339 -0.06140921\n",
      "  0.09122611 -0.09666857  0.22963035 -0.23812956  0.41410267  0.27602303\n",
      "  0.06187889  0.13667936  0.43869799  0.22540717 -0.24832362 -0.05650999\n",
      "  0.09237753 -0.3220208  -0.06862474  0.17165615  0.01730826 -0.02327888\n",
      "  0.23032499 -0.11491307 -0.09993472 -0.36805464 -0.36723119 -0.05240459\n",
      "  0.04109421 -0.20048068  0.07571288  0.10493425  0.20926017 -0.25280956\n",
      "  0.21045809  0.13050429 -0.04299298  0.02809695 -0.37234964  0.09960366\n",
      " -0.09197245  0.19826327  0.4490659  -0.24627341 -0.6563333  -0.10529582\n",
      "  0.00876969 -0.10927424 -0.04024479  0.0272919   0.05732232 -0.04287869\n",
      " -0.16631389 -0.16930741  0.19467516 -0.01984247  0.11055412  0.07805788\n",
      "  0.28784686 -0.04692793  0.18360411 -0.35668615  0.05113042  0.73329382\n",
      " -0.21286872  0.11557929  0.05401624  0.29166091 -0.23757989 -0.07499152\n",
      " -0.50733822 -0.10700433 -0.26637515  0.04376399 -0.04143217  0.47387075\n",
      "  0.04995915 -0.18716249 -0.2665481  -0.34643152 -0.02073576  0.35145645\n",
      "  0.07140786  0.08760147 -0.36089198 -0.27905731  0.20091205  0.17186928\n",
      "  0.15932176 -0.13988339 -0.10218616 -0.28237274 -0.01327303  0.30442679\n",
      " -0.06409441 -0.02704128 -0.34698997 -0.09952172 -0.39666965 -0.05765047\n",
      " -0.30927865  0.04284495 -0.21785825  0.09193266  0.015188   -0.14505224\n",
      "  0.1283548  -0.35740635 -0.27260938  0.40696038  0.19568888  0.20163153\n",
      " -0.5626199  -0.12378454  0.01835968  0.15866385 -0.16034323  0.21592091\n",
      "  0.06164871  0.11532809 -0.09130147 -0.01548833 -0.14880148  0.07873028\n",
      " -0.21104084  0.1006638   0.33749969  0.29448755  0.41263905 -0.14158221\n",
      "  0.00328937 -0.07272647 -0.05443542 -0.10597144  0.07283177  0.05165879\n",
      "  0.22693924  0.26935792  0.02776207  0.13810497 -0.59206825  0.07254165\n",
      "  0.01311433 -0.02173568]\n"
     ]
    }
   ],
   "source": [
    "print(embedding[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train a model to be stored at: SIF_pubmed+wiki+pitts-nopunct-lower-cbow-n10_MIMIC_100.joblib\n"
     ]
    }
   ],
   "source": [
    "# SIF filename\n",
    "SIF_JOBLIB_FILE_NAME = 'SIF_{0}_MIMIC_{1}.joblib'.format(os.path.splitext(os.path.basename(wordfile))[0],\n",
    "                                                        MAX_MIMIC_DOCUMENTS_FOR_TRAINING)\n",
    "\n",
    "print('Preparing to train a model to be stored at: {}'.format(SIF_JOBLIB_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17626167  0.23711565  0.08537466 -0.01195529  0.12861061 -0.13272128\n",
      " -0.19920165  0.08112462  0.05974769 -0.01452875  0.44087513 -0.43310128\n",
      " -0.05488625 -0.02537395  0.19309921 -0.13808011  0.31721966 -0.53458463\n",
      "  0.05382784 -0.32087492 -0.23657945  0.18079587  0.17358089 -0.24244937\n",
      "  0.01254969  0.1371328   0.22613152  0.07706336  0.11525739  0.10930948\n",
      " -0.59057764  0.37499111  0.30563124 -0.30139809 -0.25504244  0.18107983\n",
      " -0.25743415  0.46330456  0.22119576 -0.18447893  0.46111501  0.30915154\n",
      "  0.03020038  0.19233093 -0.0119602  -0.05628163 -0.10377176 -0.21680597\n",
      " -0.28717111  0.19528575 -0.26908734  0.16856631 -0.14822798  0.00387527\n",
      " -0.29648486  0.10642594 -0.01181775  0.23163724 -0.07085339 -0.06140921\n",
      "  0.09122611 -0.09666857  0.22963035 -0.23812956  0.41410267  0.27602303\n",
      "  0.06187889  0.13667936  0.43869799  0.22540717 -0.24832362 -0.05650999\n",
      "  0.09237753 -0.3220208  -0.06862474  0.17165615  0.01730826 -0.02327888\n",
      "  0.23032499 -0.11491307 -0.09993472 -0.36805464 -0.36723119 -0.05240459\n",
      "  0.04109421 -0.20048068  0.07571288  0.10493425  0.20926017 -0.25280956\n",
      "  0.21045809  0.13050429 -0.04299298  0.02809695 -0.37234964  0.09960366\n",
      " -0.09197245  0.19826327  0.4490659  -0.24627341 -0.6563333  -0.10529582\n",
      "  0.00876969 -0.10927424 -0.04024479  0.0272919   0.05732232 -0.04287869\n",
      " -0.16631389 -0.16930741  0.19467516 -0.01984247  0.11055412  0.07805788\n",
      "  0.28784686 -0.04692793  0.18360411 -0.35668615  0.05113042  0.73329382\n",
      " -0.21286872  0.11557929  0.05401624  0.29166091 -0.23757989 -0.07499152\n",
      " -0.50733822 -0.10700433 -0.26637515  0.04376399 -0.04143217  0.47387075\n",
      "  0.04995915 -0.18716249 -0.2665481  -0.34643152 -0.02073576  0.35145645\n",
      "  0.07140786  0.08760147 -0.36089198 -0.27905731  0.20091205  0.17186928\n",
      "  0.15932176 -0.13988339 -0.10218616 -0.28237274 -0.01327303  0.30442679\n",
      " -0.06409441 -0.02704128 -0.34698997 -0.09952172 -0.39666965 -0.05765047\n",
      " -0.30927865  0.04284495 -0.21785825  0.09193266  0.015188   -0.14505224\n",
      "  0.1283548  -0.35740635 -0.27260938  0.40696038  0.19568888  0.20163153\n",
      " -0.5626199  -0.12378454  0.01835968  0.15866385 -0.16034323  0.21592091\n",
      "  0.06164871  0.11532809 -0.09130147 -0.01548833 -0.14880148  0.07873028\n",
      " -0.21104084  0.1006638   0.33749969  0.29448755  0.41263905 -0.14158221\n",
      "  0.00328937 -0.07272647 -0.05443542 -0.10597144  0.07283177  0.05165879\n",
      "  0.22693924  0.26935792  0.02776207  0.13810497 -0.59206825  0.07254165\n",
      "  0.01311433 -0.02173568]\n"
     ]
    }
   ],
   "source": [
    "sif_model = SIFModel()\n",
    "\n",
    "# now let's train it...\n",
    "# def fit (self, sentences, We, lowercase_tokens, embeddings_type, embeddings_filepath, params, word_map, weight4ind)\n",
    "model_embeddings = sif_model.fit(training_sentences, We, LOWERCASE_TOKENS, EMBEDDINGS_FORMAT, wordfile,\n",
    "                                 sif_params, words, weight4ind)\n",
    "print(model_embeddings[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SIF_pubmed+wiki+pitts-nopunct-lower-cbow-n10_MIMIC_100.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sif_model.save(SIF_JOBLIB_FILE_NAME)\n",
    "sklearn_joblib.dump(sif_model, SIF_JOBLIB_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_sif_model = SIFModel()\n",
    "#loaded_sif_model.load(SIF_JOBLIB_FILE_NAME)\n",
    "loaded_sif_model = sklearn_joblib.load(SIF_JOBLIB_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17626167  0.23711565  0.08537466 -0.01195529  0.12861061 -0.13272128\n",
      "  -0.19920165  0.08112462  0.05974769 -0.01452875  0.44087513 -0.43310128\n",
      "  -0.05488625 -0.02537395  0.19309921 -0.13808011  0.31721966 -0.53458463\n",
      "   0.05382784 -0.32087492 -0.23657945  0.18079587  0.17358089 -0.24244937\n",
      "   0.01254969  0.1371328   0.22613152  0.07706336  0.11525739  0.10930948\n",
      "  -0.59057764  0.37499111  0.30563124 -0.30139809 -0.25504244  0.18107983\n",
      "  -0.25743415  0.46330456  0.22119576 -0.18447893  0.46111501  0.30915154\n",
      "   0.03020038  0.19233093 -0.0119602  -0.05628163 -0.10377176 -0.21680597\n",
      "  -0.28717111  0.19528575 -0.26908734  0.16856631 -0.14822798  0.00387527\n",
      "  -0.29648486  0.10642594 -0.01181775  0.23163724 -0.07085339 -0.06140921\n",
      "   0.09122611 -0.09666857  0.22963035 -0.23812956  0.41410267  0.27602303\n",
      "   0.06187889  0.13667936  0.43869799  0.22540717 -0.24832362 -0.05650999\n",
      "   0.09237753 -0.3220208  -0.06862474  0.17165615  0.01730826 -0.02327888\n",
      "   0.23032499 -0.11491307 -0.09993472 -0.36805464 -0.36723119 -0.05240459\n",
      "   0.04109421 -0.20048068  0.07571288  0.10493425  0.20926017 -0.25280956\n",
      "   0.21045809  0.13050429 -0.04299298  0.02809695 -0.37234964  0.09960366\n",
      "  -0.09197245  0.19826327  0.4490659  -0.24627341 -0.6563333  -0.10529582\n",
      "   0.00876969 -0.10927424 -0.04024479  0.0272919   0.05732232 -0.04287869\n",
      "  -0.16631389 -0.16930741  0.19467516 -0.01984247  0.11055412  0.07805788\n",
      "   0.28784686 -0.04692793  0.18360411 -0.35668615  0.05113042  0.73329382\n",
      "  -0.21286872  0.11557929  0.05401624  0.29166091 -0.23757989 -0.07499152\n",
      "  -0.50733822 -0.10700433 -0.26637515  0.04376399 -0.04143217  0.47387075\n",
      "   0.04995915 -0.18716249 -0.2665481  -0.34643152 -0.02073576  0.35145645\n",
      "   0.07140786  0.08760147 -0.36089198 -0.27905731  0.20091205  0.17186928\n",
      "   0.15932176 -0.13988339 -0.10218616 -0.28237274 -0.01327303  0.30442679\n",
      "  -0.06409441 -0.02704128 -0.34698997 -0.09952172 -0.39666965 -0.05765047\n",
      "  -0.30927865  0.04284495 -0.21785825  0.09193266  0.015188   -0.14505224\n",
      "   0.1283548  -0.35740635 -0.27260938  0.40696038  0.19568888  0.20163153\n",
      "  -0.5626199  -0.12378454  0.01835968  0.15866385 -0.16034323  0.21592091\n",
      "   0.06164871  0.11532809 -0.09130147 -0.01548833 -0.14880148  0.07873028\n",
      "  -0.21104084  0.1006638   0.33749969  0.29448755  0.41263905 -0.14158221\n",
      "   0.00328937 -0.07272647 -0.05443542 -0.10597144  0.07283177  0.05165879\n",
      "   0.22693924  0.26935792  0.02776207  0.13810497 -0.59206825  0.07254165\n",
      "   0.01311433 -0.02173568]]\n"
     ]
    }
   ],
   "source": [
    "loaded_embeddings = loaded_sif_model.transform(We, [training_sentences[0]])\n",
    "print(loaded_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n2c2] *",
   "language": "python",
   "name": "conda-env-n2c2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
