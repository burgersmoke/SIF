{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Purpose of this notebook is to a quick evaluation of embeddings in a SIF model to see how well they predict a few examples of n2c2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\n2c2\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\anaconda3\\envs\\n2c2\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "c:\\anaconda3\\envs\\n2c2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version : 3.4.0\n",
      "Spacy version : 2.0.18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "print('gensim version : {}'.format(gensim.__version__))\n",
    "\n",
    "import spacy\n",
    "print('Spacy version : {}'.format(spacy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "from SIF_embedding import get_weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io_w2v import load_w2v_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "#wordfile = '../data/glove.840B.300d.txt' # word vector file, can be downloaded from GloVe website\n",
    "wordfile = r'C:/temp_embeddings/pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin'\n",
    "\n",
    "#EMBEDDINGS_FORMAT = 'GLOVE'\n",
    "EMBEDDINGS_FORMAT = 'WORD2VEC_BIN'\n",
    "\n",
    "LOWERCASE_TOKENS = True\n",
    "\n",
    "weightfile = '../auxiliary_data/enwiki_vocab_min200.txt' # each line is a word and its frequency\n",
    "\n",
    "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "rmpc = 1 # number of principal components to remove in SIF weighting scheme\n",
    "sentences = ['this is an example sentence', 'this is another sentence that is slightly longer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec formatted embeddings from [C:/temp_embeddings/pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin] with binary=True\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load word vectors\n",
    "#(words, We) = data_io.getWordmap(wordfile)\n",
    "\n",
    "words = None\n",
    "We = None\n",
    "if EMBEDDINGS_FORMAT == 'GLOVE':\n",
    "    print('Loading embeddings as GLOVE')\n",
    "    (words, We) = data_io.load_glove_word_map(wordfile)\n",
    "elif EMBEDDINGS_FORMAT == 'WORD2VEC_BIN':\n",
    "    (words, We) = load_w2v_word_map(wordfile, binary = True)\n",
    "elif EMBEDDINGS_FORMAT == 'WORD2VEC_TXT':\n",
    "    (words, We) = load_w2v_word_map(wordfile, binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load word weights\n",
    "word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
    "weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "sif_params = params.params()\n",
    "sif_params.rmpc = rmpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SIFModel(object):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "        self.svd = None\n",
    "        self.word_map = None\n",
    "        self.params = params\n",
    "        \n",
    "    def save(self, filename):\n",
    "        components = [self.word_map, self.weight4ind, self.params, self.svd]\n",
    "        joblib.dump(components, filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        components = joblib.load(filename)\n",
    "        self.trained = True\n",
    "        self.word_map = components[0]\n",
    "        self.weight4ind = components[1]\n",
    "        self.params = components[2]\n",
    "        self.svd = components[3]\n",
    "\n",
    "    def transform(self, We, sentences):\n",
    "        x, m = data_io.sentences2idx(sentences, self.word_map) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "        w = data_io.seq2weight(x, m, self.weight4ind) # get word weights\n",
    "        weighted_emb = get_weighted_average(We, x, w)\n",
    "        # now use the model we've already loaded\n",
    "        return self.remove_pc(weighted_emb)\n",
    "        \n",
    "    def compute_pc(self, X):\n",
    "        # this is what happens in compute_pc() in src/SIF_embedding.py\n",
    "        self.svd = TruncatedSVD(n_components=self.params.rmpc, n_iter=7, random_state=0)\n",
    "        self.svd.fit(X)\n",
    "        \n",
    "    def remove_pc(self, X):\n",
    "        pc = self.svd.components_\n",
    "        \n",
    "        if self.params.rmpc == 1:\n",
    "            XX = X - X.dot(pc.transpose()) * pc\n",
    "        else:\n",
    "            XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "            \n",
    "        return XX\n",
    "        \n",
    "    def fit(self, sentences, We, params, word_map, weight4ind):\n",
    "        \n",
    "        # store these off for pickling or extra transforms\n",
    "        self.word_map = word_map\n",
    "        self.weight4ind = weight4ind\n",
    "        self.params = params\n",
    "        \n",
    "        x, m = data_io.sentences2idx(training_sentences, self.word_map) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "        w = data_io.seq2weight(x, m, self.weight4ind) # get word weights\n",
    "        \n",
    "        # now let's do some of what happens in src/SIF_embedding.py\n",
    "        # but also keep some pieces along the way\n",
    "        weighted_emb = get_weighted_average(We, x, w)\n",
    "        \n",
    "        self.compute_pc(weighted_emb)\n",
    "        \n",
    "        self.trained = True\n",
    "        \n",
    "        return self.remove_pc(weighted_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train a model to be stored at: SIF_pubmed+wiki+pitts-nopunct-lower-cbow-n10_MIMIC_100.joblib\n"
     ]
    }
   ],
   "source": [
    "# SIF filename\n",
    "# TODO : Might need to change this to include the number of MIMIC documents used for training\n",
    "SIF_JOBLIB_FILE_NAME = 'SIF_{0}_MIMIC_100.joblib'.format(os.path.splitext(os.path.basename(wordfile))[0])\n",
    "\n",
    "print('Preparing to train a model to be stored at: {}'.format(SIF_JOBLIB_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIF model loaded...\n"
     ]
    }
   ],
   "source": [
    "loaded_sif_model = SIFModel()\n",
    "loaded_sif_model.load(SIF_JOBLIB_FILE_NAME)\n",
    "\n",
    "print('SIF model loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    # 5\n",
    "    \"Albuterol [PROVENTIL/VENTOLIN] 90 mcg/Act HFA Aerosol 2 puffs by inhalation every 4 hours as needed.\",\n",
    "    \"Albuterol [PROVENTIL/VENTOLIN] 90 mcg/Act HFA Aerosol 1-2 puffs by inhalation every 4 hours as needed #1 each.\",\n",
    "\n",
    "    # 4\n",
    "    \"Discussed goals, risks, alternatives, advanced directives, and the necessity of other members of the surgical team participating in the procedure with the patient.\",\n",
    "    \"Discussed risks, goals, alternatives, advance directives, and the necessity of other members of the healthcare team participating in the procedure with the patient and his mother\",\n",
    "    \n",
    "\n",
    "    # 3\n",
    "    \"Cardiovascular assessment findings include heart rate normal, Heart rhythm, atrial fibrillation with controlled ventricular response.\",\n",
    "    \"Cardiovascular assessment findings include heart rate, bradycardic, Heart rhythm, first degree AV Block.\",\n",
    "\n",
    "    # 2\n",
    "    \"Discussed risks, goals, alternatives, advance directives, and the necessity of other members of the healthcare team participating in the procedure with (patient) (legal representative and others present during the discussion).\",\n",
    "    \"We discussed the low likelihood that a blood transfusion would be required during the postoperative period and the necessity of other members of the surgical team participating in the procedure.\",\n",
    "    \n",
    "    # 1\n",
    "    \"No: typical 'cold' symptoms; fever present (greater than or equal to 100.4 F or 38 C) or suspected fever; rash; white patches on lips, tongue or mouth (other than throat); blisters in the mouth; swollen or 'bull' neck; hoarseness or lost voice or ear pain\",\n",
    "    \"New wheezing or chest tightness, runny or blocked nose, or discharge down the back of the throat, hoarseness or lost voice.\",\n",
    "    \n",
    "    # 0\n",
    "    \"The risks and benefits of the procedure were discussed, and the patient consented to this procedure.\",\n",
    "    \"The content of this note has been reproduced, signed by an authorized physician in the space above, and mailed to the patient's parents, the patient's home care company.\"\n",
    "]\n",
    "\n",
    "if LOWERCASE_TOKENS:\n",
    "    for i in range(len(example_sentences)):\n",
    "        example_sentences[i] = example_sentences[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = loaded_sif_model.transform(We, example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  #corr = np.inner(features, features)\n",
    "  corr = features\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr*5,\n",
    "      xticklabels=[label[:20] for label in labels],\n",
    "      yticklabels=[label[:20] for label in labels],\n",
    "      vmin=0,\n",
    "      vmax=5,\n",
    "      cmap=\"YlOrRd\",\n",
    "      annot=True)\n",
    "  g.set_xticklabels([label[:20] for label in labels], rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_distances = pairwise_distances(embeddings, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity = 1.0 - cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.97714261 -0.20157387 -0.16424356 -0.09485491 -0.18215428\n",
      "  -0.08659384 -0.14764425 -0.02033581  0.09352898 -0.16578681  0.0303608 ]\n",
      " [ 0.97714261  1.         -0.12345239 -0.08047377 -0.10153267 -0.19339089\n",
      "  -0.00264245 -0.04625375 -0.05389073  0.0837525  -0.07283308  0.08302014]\n",
      " [-0.20157387 -0.12345239  1.          0.79928797  0.14324345  0.23388289\n",
      "   0.85455086  0.76553973 -0.23010395 -0.2937411   0.62186199  0.37004563]\n",
      " [-0.16424356 -0.08047377  0.79928797  1.          0.01040292  0.11804562\n",
      "   0.89427288  0.65686992 -0.20996227 -0.13202034  0.65938657  0.52579441]\n",
      " [-0.09485491 -0.10153267  0.14324345  0.01040292  1.          0.84176858\n",
      "   0.0533816   0.17023366 -0.285808   -0.2422152   0.15976527 -0.14500048]\n",
      " [-0.18215428 -0.19339089  0.23388289  0.11804562  0.84176858  1.\n",
      "   0.1380018   0.18195833 -0.2208962  -0.27861474  0.15374508 -0.02763706]\n",
      " [-0.08659384 -0.00264245  0.85455086  0.89427288  0.0533816   0.1380018\n",
      "   1.          0.71388947 -0.15601992 -0.14726514  0.55562778  0.53417242]\n",
      " [-0.14764425 -0.04625375  0.76553973  0.65686992  0.17023366  0.18195833\n",
      "   0.71388947  1.         -0.25793905 -0.25037462  0.64443261  0.25522755]\n",
      " [-0.02033581 -0.05389073 -0.23010395 -0.20996227 -0.285808   -0.2208962\n",
      "  -0.15601992 -0.25793905  1.          0.45525269 -0.25312052 -0.12725402]\n",
      " [ 0.09352898  0.0837525  -0.2937411  -0.13202034 -0.2422152  -0.27861474\n",
      "  -0.14726514 -0.25037462  0.45525269  1.         -0.27446794 -0.02233781]\n",
      " [-0.16578681 -0.07283308  0.62186199  0.65938657  0.15976527  0.15374508\n",
      "   0.55562778  0.64443261 -0.25312052 -0.27446794  1.          0.34430019]\n",
      " [ 0.0303608   0.08302014  0.37004563  0.52579441 -0.14500048 -0.02763706\n",
      "   0.53417242  0.25522755 -0.12725402 -0.02233781  0.34430019  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_similarity(example_sentences, cosine_similarity, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n2c2] *",
   "language": "python",
   "name": "conda-env-n2c2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
